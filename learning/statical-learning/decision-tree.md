---
layout: page
title: 决策树
permalink: /decision-tree/
tags: learning
---

Description:

学习笔记：[《统计学习方法笔记》](/tjxxff)第五章 决策树

<br>
### 1、决策树的模型和学习
决策树：对实例的某一特征进行测试，根据测试结果将实例分配到其子节点，得到一个与训练数据矛盾较小的树形结构，本质是从数据集中归纳出一组分类规则。    
熵是表示随机变量不确定性的度量。设X是一个取有限个值的离散随机变量，其概率分布为：    
$$
P(X=x_i)=p_i , i=1,2,...,n 
$$
则X的熵定义为：    
$$
H(x)=-\sum_{i=1}^{n} p_i \log p_i
$$

例 \\(P(X=1)=p,P(X=0)=1-p,0<=p<=1\\)时：    
$$
H(p)=-p \log_2 p - (1-p) \log_2 (1-p)
$$
熵\\(H(p)\\)随概率\\(p\\)的变化曲线在\\(p=0.5\\)的时候取最大值，不确定性最大，\\(p=0\\)或\\(p=1\\)的时候完全没有确定性。    
设有随机变量(X,Y),其联合概率分布为：    
$$
P(X=x_i,Y=y_j)=p_{ij} , i=1,2,...,n; j=1,2,...,m
$$    

条件熵\\(H(Y|X)\\)表示在已知变量\\(X\\)的条件下随机变量\\(Y\\)的不确定性，定义为\\(Y\\)在条件\\(X\\)下的条件概率分布的熵对\\(X\\)的数学期望：    

$$
H(Y|X)=\sum_{i=1}^{n} p_i H(Y|X=x_i) , i=1,2,...,n
$$

互信息：特征\\(A\\)对训练数据\\(D\\)的信息增益\\(g(D,A)\\)定义为集合D的经验熵与特征\\(A\\)给定条件下\\(D\\)的经验条件熵之差，即：    
$$
g(D,A)=H(D)-H(D|A)
$$

信息增益的算法：    
计算特征\\(A\\)对数据集\\(D\\)的经验条件熵：   
 
$$
H(D|A)= \sum\_{i=1}^{n} \frac{|D_i|}{|D|}H(D_i) = - \sum\_{i=1}^{n} \frac{|D_i|}{|D|} \sum\_{k=1}^{K} \frac{|D\_{ik}|}{|D_i|} \log_2 \frac{|D\_{ik}|}{|D_i|}
$$

信息增益比：    
$$
g_R(D,A)= \frac{g(D,A)}{H_A(D)}=\frac{H(D)-H(D|A)}{H_A(D)}
$$

<br>
### 2.决策树的生成和剪枝
\\(ID3\\)算法：相当于用极大似然估计进行概率模型的选择，对节点计算所有特征的信息增益，选择信息增益最大的特征作为节点特征，然后递归构建下一个节点    
\\(C4.5\\)算法：对\\(ID3\\)算法进行改进，每次选取增益比最大的特征作为节点，增益比小于阈值时递归结束    
极小化决策树函数的损失函数：    

$$
C_a(T)=\sum\_{t=1}^{|T|}N_tH_t(T)+\alpha|T|=-\sum\_{t=1}^{|T|}\sum\_{k=1}^{K}N\_{tk}\log\frac{N_tk}{N_t}+\alpha|T|
$$    

\\(\alpha\\)的取值决定了构建决策树的复杂度，\\(\alpha\\)取值越大，决策树的熵应越小，模型越简单，\\(\alpha\\)取值越小，对应模型越复杂，但是更容易过拟合    
剪枝：选择损失函数最小的模型。    
算法：确定\\(\alpha\\)后，从叶节点往上递归，若 \\(C\_{\alpha}(T')\geq C\_{\alpha}(T_{father}')\\) 则删掉当前节点，令其父节点为新的叶节点    

<br>
### 3.CART算法
Classification and regression tree    

1.回归树的生成    
假设输入空间划分为\\(M\\)个单元，并且在每个单元\\(R_m\\)上都有固定输出\\(c_m\\),回归树模型定义为：    
$$
f(x)=\sum\_{m=1}^{M}c_mI(x \in R_m)
$$    

单元\\(R_m\\)上的\\(c_m\\)的最优值\\(\hat{c}\_m\\)是\\(R\\)上所有实例输出的均值：    

$$
\hat{c}_m=ave(y_i|x_i \in R_m)
$$    
那么我们需要怎样的划分输入空间呢，我们假设选择第\\(j\\)个变量和它的取值\\(s\\)作为切分变量和切分点，于是可以得到两个空间\\( R_1(j,s)=\\{x|x \leq s\\}，R_2(j,s)=\\{x|x>s\\}\\)    
然后我们要怎么选取\\(j\\)和\\(s\\)呢，平方误差最小化：    

$$
\min\_{j,s}[\min\_{c_1}\sum\_{x_i \in R_1(j,s)}(y_i-c_i)^{2}+\min\_{c_2}\sum\_{x_i \in R_2(j,s)}(y_i-c_i)^{2}]
$$    

算法：遍历所有的\\(j\\)，找到最优的 (\\(j\\),\\(s\\))，划分区域，反复    

2.分类树的生成    
基尼指数定义为：    

$$
Gini(p)=\sum\_{k=1}^{K}p_k(1-p_k)=1-\sum\_{k=1}^{K}p_k^{2}
$$    

如果样本集合\\(D\\)根据特征\\(A\\)被分割为\\(D=D_1+D_2\\)(之所以分类树是二叉树而不是多叉树，是因为前面切分之后得到了两个空间，所以这里根据特征分类没必要扩展到\\(N_A\\)种特征)，那么在特征\\(A\\)的条件下集合\\(D\\)的基尼指数为：    
$$
Gini(D,A)=\frac{|D_1|}{|D|} Gini(D_1)+\frac{|D_2|}{|D|} Gini(D_2)
$$    
\\(Gini(D)\\)表示\\(D\\)的不确定性，\\(Gini(D,A)\\)表示集合\\(D\\)在特征\\(A\\)的分割下的不确定性    

\\(CART\\) 生成算法：    
计算训练数据集\\(D\\)对所有特征\\(A\\)下的基尼指数，在所有特征\\(A\\)和切分点\\(a\\)中选出最优特征和最优切分点，生成子节点，反复。    

\\(CART\\) 剪枝：    
子树的损失函数为：\\( C\_{\alpha}(T)=C(T)+\alpha|T| \\)，对于固定的 \\(\alpha\\)一定存在使损失函数最小的子树 \\(T\_{\alpha}\\)，如果最优子树是唯一的，那么\\(\alpha\\) 越大的时候，\\(T\_{\alpha}\\)越小。极限情况，\\(\alpha\\)为0，整体树最优；\\( \alpha \to \infty\\)时，根节点组成的单节点树最优。    
从整体树 \\(T_0\\) 开始剪枝，对 \\(T_0\\) 内的任意节点 \\(t\\)，以 \\(t\\) 为单节点树的损失函数是 \\( C\_{\alpha}(t)=C(t)+\alpha \\)，以  \\(t\\) 为根节点的子树 \\(T_t\\)损失函数为：\\(C\_{\alpha}(T_t)=C(T_t)+\alpha|T_t|\\).    
当 \\(\alpha=0\\) 及 \\(\alpha\\)充分小时，有 \\( C\_{\alpha}(T_t) < C\_{\alpha}(t)\\)    
当 \\(\alpha \\) 增大时，存在一个 \\(\alpha\\) 使得 \\(C\_{\alpha}(T_t) = C\_{\alpha}(t)\\)    
当 \\(\alpha \\) 继续增大时，\\( C\_{\alpha}(T_t) > C\_{\alpha}(t)\\)    
那么使 \\( \alpha = \frac{C(t)-C(T_t)}{|T_t|-1} \\)， \\(t\\) 比 \\(T_t\\)更可取，对 \\(T_t\\) 进行剪枝    
算法：初始 \\(\alpha=+\infty\\);自上而下访问所有子节点，\\(\alpha=min(\alpha,\frac{C(t)-C(T_t)}{|T_t|-1})\\)，如果有\\(g(t)=\alpha\\)进行剪枝，并对其以多数表决法决定该叶节点的类    
<br>
